\chapter{Introduction}
\label{sec:intro}
Automated specification mining or model inference \cite{lo2011mining} is the process of automatically reverse engineering a model of an existing software system. Behavioral models (e.g., state machines) are typically inferred from a running system by abstracting the execution traces. The inferred models are useful artifacts in many use cases where the actual behavior (abstracted as the inferred model) of the system is needed for analysis, such as debugging \cite{hybriddebugging, shang2013assisting, jafar2019interactive}, testing \cite{Walkinshaw2018TestingBlackBox, ModelBasedTesting, Papadopoulos2015, dallmeier2011automatically}, anomalous behavior detection \cite{valdes2000adaptive}, and requirements engineering \cite{damas2005generating}. 

Inferring a behavior model of a system in a black-box manner is particularly interesting. In many real-world applications, the large-scale system is built by integrating many off-the-shelf libraries that are only available as binaries (no source code access). Thus, from a system's point of view, knowing the exact behavior of the system including all the interactions between black-box units are needed for most run-time analysis. 

Model inference techniques are generally grouped into static and dynamic analysis categories. 
Static approaches use the code as their input data. They are able to gather all the required information such as function call graphs for example from the source code itself without a need to run the software.
A typical dynamic model inference pipeline, on the other hand, starts with running existing tests in the software to collect required data \cite{Papadopoulos2015}.


Most current behavioral model inference techniques are dynamic analysis methods (usually are more accurate than static analysis for run-time behavior inference) that require source code instrumentation to collect execution traces \cite{lo2011mining}. A common theme in them is that they use the data collected as the system functions in the wild. They either run existing test cases or instrument and inspect the system being used in production. 
This is to have a diverse collection of data that is also meaningful and representative of the actual system behaviour in common use cases. 
%Depending on the system under study and the type of data required, this might be an excellent way of collecting the data or might be infeasible for scalability reasons for example. That is what I encountered earlier while applying white-box model inference approaches to the industry partner's code base. The results are published in an ICPC 2019 paper ; to summarize that paper, the state of the art white box model inference algorithms suffer from not being scalable to be used in industry as is.
%In this study in particular there is an amalgam of both ends of the spectrum. 
These methods are usually helpful in unit-level analysis where the instrumentation is not expensive and access to the code is allowed for the unit under study. However, in the system-level, thorough instrumentation is more expensive (even prohibitively expensive \cite{mashhadi2019empirical}) and might not be even possible for some units (black-box libraries). Therefore, for use cases such as system-level anomaly detection, testing, and debugging a black-box behavior model inference that works on readily available input/outputs of the system is crucial. 

%. Most existing techniques are white-box and require access to source code either for the static analysis or instrumentation (collecting execution traces) in the case dynamic analysis. However access to source code is not always available for all libraries and component in the software) for black-box systems, which are the focus of this paper, depending on static analysis, which requires analysis of the source code, does not work. Dynamic approaches require executing the system and collecting an execution trace. These traces can include many different data, depending on their availability and use. Therefore, for a black-box model inference, the only information that is available is the input/output data.

In this thesis, I propose a dynamic analysis method to detect the internal state and the state changes in a black-box software system using deep learning. I collected the numerical values of the inputs and outputs of the system, in regular time intervals to create a multivariate time-series. A hybrid deep learning model (including convolution and recurrent layers) was then trained on these time-series to predict the state of the system at each point in time. The deep learning model automatically performs feature extraction making it way more effective and flexible compared to traditional methods. In addition, I do not make any assumption about statistical properties of the data which makes it applicable to a wide range of subjects.   


% In this study the goal is to infer a state model of the system under study as a black-box without access to its source code. Therefore I am limited to observing the inputs and outputs of the system. Another piece of information that is often overlooked is time. In this study time is taken into consideration as well. I capture all the inputs and output values of the system in regular intervals. Since they are all numeric, they make a multivariate time series that I use to generate time-aware state models. 


I applied and evaluated this method on an autopilot software used in an Unmanned Aerial Vehicle (UAV) system developed by our industry partner, Winnipeg based Micropilot Inc. Micropilot is the world-leader in professional UAV autopilot which develops both hardware and software for 1000+ clients (including NASA, Raytheon, and Northrop Grumman) in 85+ countries during the past 20+ years. In addition to that, I further replicated the results on another highly capable and widely used autopilot, Paparazzi \cite{hattenberger2014using}, as well.
I evaluated the method from two perspectives: how well the model can detect the point in time when a state change happens? (RQs 1 and 3: Change Point Detection (CPD)), and how accurately it can predict which state the system is in, during the execution? (RQs 2 and 4: State Classification). I also explored the possibilities of improving the results via hyper-parameter tuning in RQ 5.



Comparing the proposed approach with state-of-the-art alternatives, the results show that it is better in both change point and state detection. In one case study I observed 88.00\% to 102.20\% improvement in the F1 score of this CPD, compared to traditional CPD techniques. In addition, I saw a 7.35\% to 16.83\% improvement in the F1 score of state detection, compared to traditional classification algorithms on a sliding window, over the data. The same number for the other case study were 12\% to 49\% improvement in CPD and 18\% to 274\% improvement in state detection.


The contributions of this thesis can be summarised as:
\begin{itemize}
    \item Introducing the first (to the best of my knowledge) deep learning architecture to infer behavior models from black-box software systems.
    \item Empirically evaluating the model and achieving very high accuracy compared to baselines using two real-world and large-scale case studies on a UAV autopilot system developed by our industry partner as well as an open source UAV autopilot.
    \item Developed an automated fuzz testing tool capable of generating and executing test cases for Paparazzi autopilot.
    \item Created a hyper-parameter tuning pipeline to optimize the performance of the deep learning model.
\end{itemize}

Note that I have made all the source code, models, execution scripts, and some of the data available online\footnote{https://github.com/sea-lab/hybrid-net}. Due to confidentiality, the Micropilot dataset cannot be shared publicly. 

The rest of this thesis is organized as follows: In chapter~\ref{sec:motivation} I explain further how and in which contexts this research can be beneficial. 
Then in chapter~\ref{sec:background} I go through some background material and related papers this work is based on. 
In chapter~\ref{sec:approach}, I present my proposed method and model. The way it was evaluated and the results are explained in chapter~\ref{sec:experiment}. 
I provide a summary of the paper briefly discuss some paths for future continuation of this work in chapter~\ref{sec:summary}.
