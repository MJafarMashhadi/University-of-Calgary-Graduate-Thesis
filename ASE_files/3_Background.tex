\section{Background} \label{sec:background}
Unlike the numerous techniques in the literature for behavior model inference \cite{lang1998results, walkinshaw2016inferring, Lo2007Mining, dallmeier2006mining} which abstract a set of execution traces into states, our approach requires consuming a multivariate time-series and detect the state changes across time and predict the exact state labels. Thus, in this section, we briefly explain the two main sets of relevant existing techniques for ``Change Point Detection'' and ``State Prediction'' in time-series that can serve as background for our approach.  
% \subsection{State Model Inference}


\subsection{Change Point Detection}
A fundamental tool in time-series data analysis is Change Point Detection (CPD). It refers to the task of finding points of abrupt change in the underlying statistical model or its parameters that could be a result of a state transition \cite{aminikhanghahi2017survey}.
%An auto-pilot software detects changes in the inputs and makes adjustments to its outputs in order to hold some invariants (predefined rules). 
%For example, if the auto-pilot is in the ``hold altitude'' mode, it monitors the altimeter's readings and when it goes out of the acceptable range, proportionate adjustments to the throttle or the nose pitch will be made to get it back to the desired altitude. This is basically how a typical feedback loop controller, such as PID or its variations, work \cite{feedbacksystemsBook}.
%When the auto-pilot's state changed from ``hold altitude'' state to ``descend to X ft'' state, the set of invariants that the auto-pilot is trying to hold are changed. It means the auto-pilot's reactions to variations in inputs will be different. In this example, a decreasing altimeter reading will not trigger an increase in the throttle anymore.
%With all these in mind, it seems plausible to use CPD algorithms to detect when states change by looking at the inputs and outputs of the auto-pilot and how they are related.
There are plenty of CPD algorithms; many of which perform effectively on a subset of CPD problems with some assumptions. The assumptions can be of various types. For example, one may assume the time series has only one input variable (univariate) \cite{fryzlewicz2014wild}, there is only one changing point \cite{bai1998testing}, or the number of change points is known beforehand \cite{lavielle2005using}, or they might assume some statistical properties on the data \cite{chen2011parametric}. These are limiting factors, since many of these assumptions do not necessarily hold in our case. CPD techniques are categorized into two main groups: a) online methods that process the data in real-time and b) offline methods that start processing the data after receiving all the values \cite{Truong2018ChangePointSurvey}. Since our model inference use case of CPD can afford waiting to collect all historical training data, we only considered offline techniques. %But we limit ourselves to multivariate techniques with no statistical assumptions on the distribution of input data that can work with unknown number of change points. 
%, listed in a recent CPD survey \cite{Truong2018ChangePointSurvey}.

In general, CPD algorithms consist of two major components: a) the search method and b) the cost function \cite{Truong2018ChangePointSurvey}.
Search methods are either exact or approximate. For instance, Pelt is the most efficient exact search method in the CPD literature, which uses pruning \cite{killick2012optimal}. Approximate methods include window-based \cite{basseville1993detection}, bottom-up \cite{keogh2001online}, binary segmentation \cite{scott1974cluster}, and more. In the window-based segmentation a sliding window is rolled over the data and then sum of costs of left and right half-windows is subtracted from the cost of the whole window. When the difference gets significantly high it means that the discrepancy between left and right half of the window is high and therefore a change point probably lies right in the middle of the window. In the bottom-up method, the input signal is split into multiple smaller parts, then using a similarity measure adjacent segments are merged until no more merges are feasible. The binary segmentation method finds one change point and splits the input into two parts around that point and then recursively applies the same method on each part.

The cost functions are also quite various, from simply subtracting each point from the mean to much more complex metrics, such as auto-regressive cost functions \cite{angelosante2012group}, and kernel-based cost functions. Kernel-based costs can have a wide variety, since the kernel function can be almost arbitrary, however a handful of them such as linear and Gaussian kernels are among the most popular ones \cite{Truong2018ChangePointSurvey}.

In the context of our paper, we need a CPD method with no assumption on data distribution, number of change points, etc. In addition, our CPD method should work on multivariate data, and be able to capture non-linear relations between signals. It also needs to be resilient to time lags between an input signal change and its effect on the output signal (and the systems state). There is no traditional CPD algorithms that covers all these requirements.
Therefore, we propose a novel CPD techniques that is based on Hybrid DNNs and compare it with several existing CPD techniques as our baselines, which are explained in details in section~\ref{sec:experiment}.   


\subsection{Convolutional and Recurrent Neural Networks}
In both our problems (CPD and state classification), we can see that the changes in signals are more informative than their absolute values. Therefore, applying a derivation operation (or more generally a gradient) seems like necessary, at some point in the processing. Farid and Simoncelli listed some discrete derivation kernels in their study \cite{Farid2004}, but to have a more generalized and more flexible notion of discrete derivatives, convolutions seems like a better choice to apply. 
Nowadays, applying convolutional filters on signals is pretty much a standard process in signal processing studies that leverage deep learning \cite{morales2016deep, zeng2014convolutional, yang2015deep}. Convolutional neural networks (CNNs) can learn to find features in a multidimensional input while being less sensitive to the exact location of the feature in the input \cite{lecun2015deep}. In the forward pass of a convolutional layer, multiple filters are applied to the input. %Each convolutional layer can learn and apply multiple filters, 
It means that in a trained neural net, multiple features can be leaned in one single convolutional layer.

Recurrent neural networks (RNN) have shown great performance in analysing sequential data such as machine translation, time-series prediction, and time-series classification \cite{cho2014learning, zhang2000predicting, wang2017time, murad2017deep, yang2015deep, Ordonez2016}. RNNs can capture long-term temporal dependencies which is quite useful for solving our problem. \cite{Che2018} For example, they might learn that ``climb'' state in a UAV auto-pilot usually follows ``take off''. Therefore, while it is outputting ``take off'' it anticipates what the next state will probably be and as soon as its input features start shifting, it detects the onset of a state change. It will help the model to better predict the system's behavior and be quicker to detect state changes in a way that could hardly be achieved with classic methods.
Therefore, in this paper, we combine the CNNs and RNNs to create what is known as a hybrid deep neural network \cite{wang2017time} to use for both CPD and state classification problems, in our context.  
% http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf

