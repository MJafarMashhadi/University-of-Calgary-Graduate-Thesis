\section{Related Work} \label{sec:related_work}

\subsection{Time Series Change Point Detection}
Change point detection is a well-studied subject due to its wide range of applications \cite{basseville1993detection}.
Several statistical and algorithmic methods have been tried to tackle several variations of this problem \cite{chen2011parametric, hasan2014information, hsu1982bayesian, lee2017implicit, oh2002analyzing, ramos2016anomalies, chowdhury2012bayesian, reeves2007review, rosenfield2010change, wang2011non, xie2013sequential, yamanishi2004line, Lavielle1999}. 
The models vary based on: whether the whole data is available at once (offline) or it is being generated on the go (online), whether there are statistical assumptions about the data distribution \cite{takeuchi2006unifying, ide2007change}, whether the number of change points is known \cite{Truong2018ChangePointSurvey}, or whether we are dealing with a univariate or a multivariate time series, etc. 

Ives and Dakos utilized locally linear models and used statistical significance test to determine at which point the changes in model parameters are large enough to signal a change in the state \cite{Ives2012}. Blythe et al., used subspace analysis to reduce data dimensionality to keep the most non-stationary dimensions. This process helps detecting change points more effectively \cite{Blythe2012}.
Several techniques have used penalty functions to find models that best fit each segment of the signal \cite{Lavielle1999, lavielle2005using, keshavarz2018optimal, pein2017heterogeneous, khan2019deep}. 
Desobry et al., and Hido et al., proposed methods to indirectly use classifiers such as SVM to detect change points \cite{desobry2005online, hido2008unsupervised, Khan2019thesis}. We applied their approach on our data in early stages of the research but it could not perform as others.
Lee et al., trained deep auto encoder networks that learns latent features in the data to detect change points \cite{Lee2018TimeSeriesSegmentation}. Ebrahimzadeh et al., proposed what they call a pyramid recurrent neural network architecture, which is resilient to missing to detect patterns that are warped in time \cite{Ebrahimzadeh2019}.

There are also a family of methods based on Bayesian models that focus on finding changes in parameters of underlying distributions of the data \cite{Lee2018TimeSeriesSegmentation, adams2007bayesian, bai1997estimation, barry1993bayesian, erdman2008fast, ray2002bayesian}. 

Making assumptions about the data such as its distribution or the distribution of change points across the time and relying on basic statistical properties are the two major short comings of traditional CPD methods \cite{Lee2018TimeSeriesSegmentation}, which our proposed approach has overcome.


\subsection{State Model Inference}
Roughly speaking, dynamic EFSM\footnote{Extended Finite State Machines, are special kind of state machines that have conditional expressions called ``transition guards'' on their transitions \cite{lorenzoli2008automatic}. A state transition can only happen if the transition guard evaluates as true.} inference algorithms generally take a trace of ``events'' (along with perhaps some variable values) as their input \cite{walkinshaw2016inferring} to infer a generalized finite state machine. They use the events to find the state transitions and the values for detecting invariants and generating the guard conditions on the transitions. 
k Tails, Gk Tail, EDSM, and MINT are examples of these algorithms, each improving upon the previous one \cite{biermann1972synthesis, lorenzoli2008automatic, lang1998results, walkinshaw2016inferring}.  % Merge paragraphs together

Walkinshaw et al., proposed an algorithm and developed a tool for state model inference \cite{walkinshaw2016inferring}. Their work is based on previous endeavors on state merging algorithms such as gk-tail and k-tails \cite{lorenzoli2008automatic, biermann1972synthesis}. These methods require an execution trace of the program consisting of a list of ``events'' that occurred during program execution, such as function calls, system calls, transmitted network data, etc. Krka et al., performed an empirical study on 4 different categories of model inference algorithms to figure out what makes each group of methods more effective \cite{krka2014automatic}. Beschastnikh et al., proposed a method to mine invariants from partially ordered logs from concurrent/distributed systems \cite{beschastnikh2011mining}. Invariants can be used to augment state models \cite{beschastnikh2014inferring, beschastnikh2011leveraging}. Groz et al., use machine learning to heuristically infer state machine models of a un-resettable black-box system \cite{groz2018revisiting}, however a significant difference between our method and theirs is that their method still relies on discrete events (such as HTTP request and responses) while our method does not assume that the input and outputs contain any kind of ``events'' happening at certain times. Our method aims to search for such events as change points in a continuous stream of data as time series. %Papadopoulos et al., proposed an active learning method (with repeated interaction with a user) to model a black-box system and generate test data \cite{Papadopoulos2015}.


\subsection{Using Deep Learning on Time Series Data} \label{sec:related_work_har}
Human activity recognition (HAR) is a well researched task which is quite relevant to the problem of black-box model inference. In HAR, just like in our context, a multivariate time series data is created from various sensors on a human body. The goal is to figure our what was the activity that human was performing in different time intervals. The sensors can be body worn accelerometers, or more generic sensors such as the ones found in a smart watch or a smartphone. 
Murad et al., \cite{murad2017deep} have shown deep RNNs outperform fully convolutional networks and deep belief networks in HAR task.
Hybrid models are the combination of some deep architectures \cite{wang2019deep}, such as a CNN + RNN or a CNN + a fully connected net. Morales et al., have shown the former preforms better than the latter in HAR \cite{morales2016deep}. Yao et al., \cite{deepsense} introduced a CNN + RNN architecture that outperforms the state of the art both in classification and in regression tasks. Similar results have been shown in other works such as \cite{Ordonez2016, singh2017transforming, zheng2016exploiting}, as well.

Another related topic here is the time series classification. However, time series classification techniques often output only one label classifying entire data, thus not applicable in our context. What is more related to our problem is called ``segmentation'', using the computer vision terminology (not be confused with time series segmentation, such as \cite{lemire2007better}). U-net is one of the promising auto-encoder architectures for image segmentation \cite{ronneberger2015u}. Perslev et al., developed a similar idea for time-series to capture long-term dependencies and called it U-time \cite{perslev2019u}. It is fully convolutional and does not use memory cells (recurrent cells). A fully convolutional model can perform very well, since convolutions operate locally and image segments are large chunks of pixels in the 2D space and capturing local features using neighbouring pixels is quite useful. However, it cannot necessarily be as powerful on a more limited 1D data of time-series with different characteristics from an image.  This study's design is optimized for the task of sleep phase detection, which does not have very clear boundaries between states and also the state changes are not very frequent. Therefore, the same method does not necessarily generalize to tasks such as ours, where we cannot make assumptions about frequency of state changes.





\section{Summary and Future Work} \label{sec:summary} % On result
In this paper, we introduced a hybrid CNN-RNN model that can be used for both CPD and state classification problems in multivariate time series. The proposed approach can be used as a black-box state model inference for variety of use cases such as testing, debugging, and anomaly detection in control software systems, where there are several input signals that control output states. We have evaluated our approach on a case study of a UAV auto-pilot software from our industry partner with 888 test cases and showed significant improvement in both change point detection and state classification. In the future, we are planning to extend this research with more case studies from open source auto-pilots. In addition, better tuning of hyper-parameters will be explored. Finally, we plan to examine the use of transfer learning to reduce the labeling overhead.
